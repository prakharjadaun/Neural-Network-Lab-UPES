{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building a Multilayer Perceptron and trainning it with backpropagation algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definning the class of the multi layer perceptron\n",
    "\n",
    "class MLP():\n",
    "    # constructor of the mlp class\n",
    "    def __init__(self,alpha,v,w):\n",
    "        self.alpha = alpha\n",
    "        self.v = v\n",
    "        self.w = w\n",
    "    # defining the binary sigmoidal activation function\n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    # appending 1 at the index of the argument passed\n",
    "    # we are appending 1 because we are adding bias to the neuron input\n",
    "    def append_bias(self,x):\n",
    "        return np.insert(x,0,1)\n",
    "\n",
    "    # computing the binary sigmoid derivative\n",
    "    def sigmoid_derivative(self,x):\n",
    "        return x*(1-x)\n",
    "\n",
    "    # function to compute the feed forward from input to hidden layer\n",
    "    def feed_forward_input_to_hidden(self,x):\n",
    "        self.z = self.sigmoid(np.matmul(self.v,x))\n",
    "        self.z = self.append_bias(self.z)\n",
    "        return self.z\n",
    "\n",
    "    # function to compute the feedforward from hidden to input layer    \n",
    "    def feedforward_hidden_to_out(self):\n",
    "        self.y = self.sigmoid(np.matmul(self.w,self.z))\n",
    "        return self.y\n",
    "        \n",
    "    # computing the error portion of the output layer\n",
    "    def error_portion(self,t):\n",
    "        self.error_p = (t-self.y)*self.sigmoid_derivative(self.y)\n",
    "        return self.error_p\n",
    "    \n",
    "    # change of weights between hidden and the output layer\n",
    "    def compute_dw(self):\n",
    "        self.dw = self.alpha*(self.error_p*self.z)\n",
    "        return self.dw\n",
    "\n",
    "    # error portion of the hidden layer\n",
    "    def error_portion_hidden(self):\n",
    "        self.error_p_hidden = []\n",
    "        for i in range(1,len(self.z)):\n",
    "            temp = (self.error_p*self.w[i])*self.sigmoid_derivative(self.z[i])\n",
    "            self.error_p_hidden.append(temp)\n",
    "        self.error_p_hidden = np.array(self.error_p_hidden)\n",
    "        return self.error_p_hidden\n",
    "    \n",
    "    # change of weights between the hidden and the input layer\n",
    "    def compute_dv(self,x):\n",
    "        self.dv = []\n",
    "        for i in range(len(self.error_p_hidden)):\n",
    "            temp = self.alpha*self.error_p_hidden[i]*x \n",
    "            self.dv.append(temp)\n",
    "        self.dv = np.array(self.dv)\n",
    "        return self.dv\n",
    "\n",
    "    def weight_update(self):\n",
    "        self.w = self.w + self.dw\n",
    "        self.v = self.v + self.dv\n",
    "        return self.w,self.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,X,T,epocs=5):\n",
    "  cost=[]\n",
    "  for epoch in range(epocs):\n",
    "    yexp=[]\n",
    "    for i in range(len(X)):\n",
    "      # 1. feed forward from input to hidden layer\n",
    "      model.feed_forward_input_to_hidden(X)\n",
    "\n",
    "      # 2. feed forward from hidden to output layer\n",
    "      yhat = model.feedforward_hidden_to_out()\n",
    "\n",
    "      # 3. computing the error portion\n",
    "      model.error_portion(T)\n",
    "        \n",
    "      # 4. finding the change of weights : dw\n",
    "      model.compute_dw()\n",
    "\n",
    "      # 5. computing the error portion of the hidden layer\n",
    "      model.error_portion_hidden()\n",
    "\n",
    "      # 6. computing the change of weights between the hidden and the input layer : dv\n",
    "      model.compute_dv(X)\n",
    "\n",
    "      # 7. updating the weights\n",
    "      model.weight_update()\n",
    "      yexp.append(yhat)\n",
    "    print(\"Epoch\",epoch,\":\",yhat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the learning rate \n",
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3,  0.6, -0.1],\n",
       "       [ 0.5, -0.3,  0.4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assigning the weights of the neuron between the input and hidden layer\n",
    "v = np.array([[0.3,0.6,-0.1],[0.5,-0.3,0.4]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2,  0.4,  0.1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assigning the weights of the neuron between the hidden and the output layer\n",
    "w = np.array([-0.2,0.4,0.1])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(alpha,v,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating tensors of x and y\n",
    "X = np.array([0,1],dtype=np.float32)\n",
    "T = np.array([1],dtype=np.float32)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = model.append_bias(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target values\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 0.6225246377668144\n",
      "Epoch 1 : 0.717972793442314\n",
      "Epoch 2 : 0.7734275573992476\n",
      "Epoch 3 : 0.8085876684073626\n",
      "Epoch 4 : 0.8327646641513959\n",
      "Epoch 5 : 0.8504453429672344\n",
      "Epoch 6 : 0.8639846867704015\n",
      "Epoch 7 : 0.8747230501301559\n",
      "Epoch 8 : 0.8834763760772626\n",
      "Epoch 9 : 0.890769302927056\n",
      "Epoch 10 : 0.8969545965769304\n",
      "Epoch 11 : 0.902278392939364\n",
      "Epoch 12 : 0.90691782945573\n",
      "Epoch 13 : 0.9110037761793701\n",
      "Epoch 14 : 0.9146351199084393\n",
      "Epoch 15 : 0.9178880505770097\n",
      "Epoch 16 : 0.9208222757683592\n",
      "Epoch 17 : 0.9234852922645259\n",
      "Epoch 18 : 0.9259153885797113\n",
      "Epoch 19 : 0.9281437956145406\n",
      "Epoch 20 : 0.9301962623486634\n",
      "Epoch 21 : 0.9320942217686405\n",
      "Epoch 22 : 0.9338556704375143\n",
      "Epoch 23 : 0.9354958392288909\n",
      "Epoch 24 : 0.93702771109221\n",
      "Epoch 25 : 0.938462429930906\n",
      "Epoch 26 : 0.9398096157593697\n",
      "Epoch 27 : 0.9410776238378684\n",
      "Epoch 28 : 0.9422737463783419\n",
      "Epoch 29 : 0.9434043810682603\n",
      "Epoch 30 : 0.9444751640393952\n",
      "Epoch 31 : 0.9454910840418129\n",
      "Epoch 32 : 0.9464565726997348\n",
      "Epoch 33 : 0.947375582142445\n",
      "Epoch 34 : 0.9482516482852777\n",
      "Epoch 35 : 0.9490879462341406\n",
      "Epoch 36 : 0.9498873356487101\n",
      "Epoch 37 : 0.9506523992447471\n",
      "Epoch 38 : 0.9513854769010452\n",
      "Epoch 39 : 0.9520886926247379\n",
      "Epoch 40 : 0.9527639806544109\n",
      "Epoch 41 : 0.953413106525483\n",
      "Epoch 42 : 0.954037684013623\n",
      "Epoch 43 : 0.9546391932521634\n",
      "Epoch 44 : 0.9552189931078305\n",
      "Epoch 45 : 0.9557783344658332\n",
      "Epoch 46 : 0.9563183698856665\n",
      "Epoch 47 : 0.9568401650637486\n",
      "Epoch 48 : 0.9573447050000018\n",
      "Epoch 49 : 0.957832902910999\n"
     ]
    }
   ],
   "source": [
    "train(model,X,T,epocs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
